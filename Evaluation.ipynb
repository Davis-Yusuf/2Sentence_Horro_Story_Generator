{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 18:24:59.578030: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Novel Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_500 = pd.read_csv(\"Reference.csv\")\n",
    "actual_model = pd.read_csv(\"Project_Model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Scores for Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.21602950275169036,\n",
       "  'p': 0.23020406010296718,\n",
       "  'f': 0.2155080248580278},\n",
       " 'rouge-2': {'r': 0.02984986660885796,\n",
       "  'p': 0.02931860284566707,\n",
       "  'f': 0.02844579864607194},\n",
       " 'rouge-l': {'r': 0.18554827921564976,\n",
       "  'p': 0.19718098036442222,\n",
       "  'f': 0.18492090426100036}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_out = actual_model['completion']\n",
    "rouge = Rouge()\n",
    "rouge.get_scores(actual_out, reference_500['completion'], avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Curie Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "curie_model = pd.read_csv(\"curie.csv\")\n",
    "reference_100 = reference_500[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Scores for Curie Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.16931688479340123,\n",
       "  'p': 0.23636990952455558,\n",
       "  'f': 0.18893383104417963},\n",
       " 'rouge-2': {'r': 0.02485209625783794,\n",
       "  'p': 0.03204883031627344,\n",
       "  'f': 0.026899192076439437},\n",
       " 'rouge-l': {'r': 0.14891694609704154,\n",
       "  'p': 0.20623464928205532,\n",
       "  'f': 0.165553420355622}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curie_out = curie_model['completion']\n",
    "rouge = Rouge()\n",
    "rouge.get_scores(curie_out, reference_100['completion'], avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Scores for Davinci Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.17204452309607685,\n",
       "  'p': 0.2278095534069902,\n",
       "  'f': 0.18878300499805933},\n",
       " 'rouge-2': {'r': 0.02235971805150675,\n",
       "  'p': 0.028695131292866674,\n",
       "  'f': 0.024214273666174586},\n",
       " 'rouge-l': {'r': 0.14947961467554996,\n",
       "  'p': 0.1943106636332181,\n",
       "  'f': 0.16274900816512441}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davinci_model = pd.read_csv(\"vanilla_davinci.csv\")\n",
    "davinci_out = davinci_model['completion']\n",
    "rouge = Rouge()\n",
    "rouge.get_scores(davinci_out, reference_100['completion'], avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Scores for Text_Davinci_002 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.19422400246160532,\n",
       "  'p': 0.23112103081245963,\n",
       "  'f': 0.20452867353770102},\n",
       " 'rouge-2': {'r': 0.02449121435985305,\n",
       "  'p': 0.027886871648139237,\n",
       "  'f': 0.025360971023925837},\n",
       " 'rouge-l': {'r': 0.17073248757226814,\n",
       "  'p': 0.20188716184198863,\n",
       "  'f': 0.17931967388857395}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_davinci_model = pd.read_csv(\"test_davinci_002.csv\")\n",
    "text_davinci_out = text_davinci_model['completion']\n",
    "rouge = Rouge()\n",
    "rouge.get_scores(text_davinci_out, reference_100['completion'], avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Scores for Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_f1 = 0\n",
    "for i in range(500):\n",
    "    predictions = [reference_500['completion'][i]]\n",
    "    references = [actual_out[i]]\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    total_precision += results['precision'][0]\n",
    "    total_recall += results['recall'][0]\n",
    "    total_f1 += results['f1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8574117990732193\n",
      "Recall:  0.8669113981723785\n",
      "F1:  0.8620482590198517\n"
     ]
    }
   ],
   "source": [
    "print('Precision: ', total_precision/500)\n",
    "print('Recall: ', total_recall/500)\n",
    "print('F1: ', total_f1/500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Scores for Curie Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "curie_precision = 0\n",
    "curie_recall = 0\n",
    "curie_f1 = 0\n",
    "for i in range(100):\n",
    "    predictions = [reference_100['completion'][i]]\n",
    "    references = [curie_out[i]]\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    curie_precision += results['precision'][0]\n",
    "    curie_recall += results['recall'][0]\n",
    "    curie_f1 += results['f1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8522293680906295\n",
      "Recall:  0.8737982106208801\n",
      "F1:  0.8627993142604828\n"
     ]
    }
   ],
   "source": [
    "print('Precision: ', curie_precision/100)\n",
    "print('Recall: ', curie_recall/100)\n",
    "print('F1: ', curie_f1/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Scores for Davinci Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "davinci_precision = 0\n",
    "davinci_recall = 0\n",
    "davinci_f1 = 0\n",
    "for i in range(100):\n",
    "    predictions = [reference_100['completion'][i]]\n",
    "    references = [davinci_out[i]]\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    davinci_precision += results['precision'][0]\n",
    "    davinci_recall += results['recall'][0]\n",
    "    davinci_f1 += results['f1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8532716172933579\n",
      "Recall:  0.871405559182167\n",
      "F1:  0.8621839964389801\n"
     ]
    }
   ],
   "source": [
    "print('Precision: ', davinci_precision/100)\n",
    "print('Recall: ', davinci_recall/100)\n",
    "print('F1: ', davinci_f1/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Scores for Text_Davinci_002 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_davinci_precision = 0\n",
    "t_davinci_recall = 0\n",
    "t_davinci_f1 = 0\n",
    "for i in range(100):\n",
    "    predictions = [reference_100['completion'][i]]\n",
    "    references = [text_davinci_out[i]]\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    t_davinci_precision += results['precision'][0]\n",
    "    t_davinci_recall += results['recall'][0]\n",
    "    t_davinci_f1 += results['f1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8575064653158188\n",
      "Recall:  0.8711072331666947\n",
      "F1:  0.8641858297586441\n"
     ]
    }
   ],
   "source": [
    "print('Precision: ', t_davinci_precision/100)\n",
    "print('Recall: ', t_davinci_recall/100)\n",
    "print('F1: ', t_davinci_f1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
